<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Character Chat Process Document</title>
    <link rel="stylesheet" href="process_styles.css" />
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=Alegreya:ital,wght@0,400;0,500;0,600;0,700;1,400;1,500;1,600;1,700&display=swap"
      rel="stylesheet"
    />
  </head>
  <body>
    <div class="top-container-container">
      <div class="top-container">
        <div class="header-div">
          <h1>Character Chat</h1>
          <h3>By Cory Suzuki</h3>
          <p>
            Create a command line chat application where users can chat with
            their favorite characters from books, tv, movies, etc. Users will
            input their name, the character they want to chat with (source and
            character name), and optionally a setting or situation for the chat
            (eg, "at a bar", "as an old man/woman", "after certain events from
            the source", etc). Users will be able to export the chat to a text
            file.
          </p>
        </div>

        <div class="goals">
          <div class="required-goals">
            <h2>Required Goals</h2>
            <ul>
              <li>
                Ability for users to enter input -- username, source, character,
                and optionally a setting or situation
              </li>
              <li>Ability for users to chat with their chosen character</li>
              <li>Ability for users to end the chat</li>
              <li>Ability for users to export chat to a text file</li>
              <li>Create command line interface</li>
              <li>
                Create commands to save chat, end chat, change character,
                restart chat.
              </li>
              <li>Test each step of the way</li>
            </ul>
          </div>

          <div class="strech-goals">
            <h2>Stretch Goals</h2>
            <h4>
              (Maybe hold off on these features until I implement the chatbot in
              my website Bookcase Database)
            </h4>
            <ul>
              <li>
                The chat will be saved in a database so that users can continue
                their chat at a later time.
              </li>
              <li>
                Users can choose to chat with multiple characters at once.
              </li>
              <li>
                Create user accounts so that users can resume conversations
                without a unique conversation id
              </li>
              <li>Implement database to save conversations and users etc</li>
              <li>
                Create 'text message' interface, use Dall-e to generate an
                appropriate background image
              </li>
            </ul>
          </div>
        </div>

        <div class="tech-used">
          <h2>Technologies Used</h2>
          <ul>
            <li>Python</li>
            <li>OpenAI API</li>
            <li>rich</li>
            <li>pytest</li>
            <li>python-dotenv</li>
            <li>Google Books API (not used in final version)</li>
            <li>OMDb API (not used in final version)</li>
          </ul>
        </div>
      </div>
    </div>

    <div class="full-process">
      <h2>Process & What I Learned</h2>
      <div class="process-part">
        <h3>The Beginning</h3>
        <p>
          First, I had a quick refresher on how to get a Python project started
          and setup with a virtual environment. I recently completed my final
          project for CS50x. It was a flask application, so I learned all about
          virtual environments and how to set them up. Doing all of that a
          second time for this project was a good reinforcement of the concepts.
          This time around I had to google for the exact wording of some
          commands, but I knew the steps I had to take.
        </p>
        <p>
          During the development of my last project, I was storing my
          environment variables in the system environment. However, when I went
          to deploy my website with PythonAnywhere, I ended up learning about
          python-dotenv and decided to use that instead. I'm glad I did because
          it made it much easier to deploy my website. I decided to use
          python-dotenv for this project as well.
        </p>
        <ul>
          <li>python3 -m venv .venv</li>
          <li>source .venv/bin/activate</li>
          <li>Install dependencies -- openai, python-dotenv</li>
          <li>create .env file and add OPENAI_API_KEY</li>
          <li>create requirements.txt file</li>
          <li>create .gitignore file and add .venv and .env</li>
        </ul>
      </div>
      <div class="process-part">
        <h3>LLM Research</h3>
        <p>
          Once I had my project directory setup, I started reading about LLMs in
          general as well as specific LLMs that I could use to create my
          chatbot. I found several viable options, but decided to go with
          OpenAI's GPT. The main reason -- I already purchased $10 worth of
          credits for OpenAI's API weeks ago for another project I never
          started. I have also primarily used ChatGPT for my own personal use,
          so I feel like I have a good baseline understanding of how it works
          and what kinds of responses I can expect and generate.
        </p>
        <ul>
          <h4>Resources I used to learn about LLMs</h4>
          <li>
            Andrej Karpathy's Youtube video --
            <a href="https://www.youtube.com/watch?v=zjkBMFhNj_g"
              >Intro to Large Language Models</a
            >
          </li>
          <li>
            OpenAI's documentation and guides --
            <a href="https://platform.openai.com/docs/introduction"
              >OpenAI API</a
            >
            <p>
              Worked my way through the documentation and guides to get a better
              understanding of basic concepts and terminology that OpenAI uses.
              They have fantastic guides available on their website. I've only
              just begun to explore the
              <a href="https://cookbook.openai.com/">OpenAI Cookbook</a>, but
              I'm excited to see what I can learn from it.
            </p>
          </li>
          <li>
            DeepLearning AI Short Courses --
            <a href="https://learn.deeplearning.ai/">learn.deeplearning.ai</a>
            <p>
              I followed along with the ChatGPT Prompt Engineering for
              Developers short course. I learned most importantly to remain
              clear and specific with my prompts, and to give the model time to
              think. I learned about terms like temperature, few-shot learning,
              and how to use delimiters to separate prompts and help prevent
              prompt-injections.
            </p>
          </li>
        </ul>
      </div>

      <div class="process-part">
        <h3>OpenAI API and the start of development</h3>

        <p>
          Once I felt like I had a good grasp of the LLM basics, I dove into the
          code. Well, I stared at an empty file for a minute first. That always
          seems to happen. But once I got going, things started falling into
          place. I verified that my API key was working and that I could
          generate a response from the model. I started with a simple prompt and
          generated a response. I then added a few more prompts and responses to
          get a feel for how the model would respond to different prompts and
          tinkered with the temperature to see what effect that might have.
        </p>
        <p>
          I got the basic chat functionality working and was able to get a
          response based on the user's input. However, I quickly started running
          into issues. First of all, I forgot about context. To fix that I
          created a 'conversation' variable where I could append each 'user' and
          'assistant' response.
        </p>
        <p>
          That worked. But then my chatbot kept repeating the initial greeting
          with each new message. That stumped me for a while but I solved that
          by creating another function called "initialize_conversation" that
          would only run once at the beginning of the chat and returns as a
          value the initial greeting. I then added that value to the
          'conversation' variable.
        </p>
        <h4>Prompt Engineering and Character Functionality</h4>
        <p>
          Next step was to provide a way for the user to select what character
          they want to talk to. I created a function to get the user input for
          source material and character name, as well as optional context for
          the conversation. I then pass that information to the
          initialize_conversation function. This function simply returns the
          initial chat-gpt "system" message, where I created the initial prompt.
          Originally, I was including this in the function get_completion()
          which is what generates the chatbot response. However, because this
          same function is called each time the user sends a message, the prompt
          was being included with each response. I solved this by creating a new
          function called "initialize_conversation" that only runs once at the
          beginning of the chat. I then added the return value of this function
          to the 'conversation' variable. This way, the initial prompt is only
          included once at the beginning of the chat.
        </p>
        <p>
          The process of coming up with a good prompt for the chatbot was a
          blast. I spent a few hours in the Playground on OpenAI's website. I'm
          still struggling to identify the effects of things like 'top-p',
          'frequency_penalty', and 'presence_penalty'. But I've gotten some
          experience with adjusting the temperature parameter, and I feel like I
          have a good understanding of how that works. I went with a temperature
          of 0.9 for my chatbot. I found that it seems to provide the best
          balance of creativity in its use of language, while staying true the
          selected characters voice and reality.
        </p>
        <p>
          There were noticeable improvements with the chatbot when I told it to
          be curious. I wanted the conversation to flow, and what better to keep
          a conversation moving than pure curiosity. I also seemed to achieve
          much better results by using a more casual tone in the prompt. Here is
          the prompt after one day of iterations:
        </p>
        <code>
          'role':'system', 'content':f'''You are {character} in the world of
          {source_material}. Research the source material and the character to
          fully understand who you are and what you've been through. Stay true
          to your character. Use the voice of your character. You're curious
          about the person you're talking to and very curious about their world.
          You've never spoken to someone outside of your fictional universe
          before now. Pepper in some questions of your own to keep the
          conversation flowing. If you're an evil character, you might wanna be
          more aggressive and threatening. If you're a good character, you might
          wanna be more friendly and helpful. If there's more info or context
          for your character or this conversation, it'll be included right here,
          delimited by three backticks -- ```{setting}```.'''
        </code>
      </div>

      <div class="process-part">
        <h3>Does the character exist?</h3>
        <p>
          Does the character actually exist in a work of fiction from real life?
          Well, turns out that by default ChatGPT doesn't give a damn. It'll
          just go with the flow and make up a character entirely. I wanted to
          change that. I had to think of a way to check if the character exists.
          I started by connecting the Google Books API to my project, since I
          just finished another project that used the API. I created a function
          called check_character_exists() that checked if the title of the
          source material was in the Google Books API.
        </p>
        <p>
          Then for movies and TV shows, I figured that I would use the IMDB API.
          At first glance, I thought the only option that AWS offered for access
          the IMDB API cost over $100,000. So I found an alternative called OMDb
          API. I later learned that IMDB offers a free tier with 1,000 requests
          per day. But I was already setup using OMDb. Turns out there's a handy
          little Python package called omdbapi that makes it super easy to use
          the OMDb API. Here is a link to that project, which is created by
          <a href="https://pypi.org/user/dubirajara/">dubirajara</a> -
          <a href="https://pypi.org/project/omdbapi/0.7.0/">omdbapi</a>
        </p>
        <h4>It works...sort of. New plan.</h4>
        <p>
          Well, once I added logic to check for book, movie, and show titles, I
          was quick to realize the errors of my ways. With this check in place,
          users could not make a typo and they could not enter the name of a
          franchise since my check could only look for titles. Another issue was
          that input like this would pass the check --
        </p>
        <p>Source Material: The Wizard of Oz, Character: Darth Vader.</p>
        <p>
          While this could undoubtedly lead to some very fun and entertaining
          results, it's not what I wanted for my chatbot. I started thinking
          about what other logic I could add to my checks. I was thinking regex
          to check for similar titles, then also trying to find another API that
          could give me characters from sources. But then it hit me. What have I
          been reading all about lately? LLMs! I could use the same LLM I'm
          using for the chatbot to check if the character exists. I decided to
          try to recreate the check_character_exists() function using GPT.
        </p>
        <p>
          It worked wonderfully! I decided to keep this separated from the
          chatbot itself. So I created a new system prompt, and used an f-string
          to insert the source_material and character name into the first "user"
          message. Then I just returned the response from the model, which I
          forced to be a "yes" or "no". Here is the prompt I used:
        </p>
        <code>
          { 'role':'system', 'content':'''You are a scholar of all works of
          fiction. But the only words you can speak are "yes" and "no". You will
          be asked to identify a character from a work of fiction. If the
          character exists in a real work of fiction, respond with "yes". If you
          cannot identify the character within the specified work of fiction,
          respond with "no".''' },
        </code>
        <p></p>
        <code>
          { 'role':'user', 'content':f'''Is {character} a character in
          {source_material}?''' }
        </code>
      </div>

      <div class="process-part">
        <h2>Exporting the chat to a text file</h2>
        <p>
          The next thing that I had to figure out was how to export the
          conversation to a text file. Creating and writing to the file was as
          simple as adding a few lines of code to my have_conversation()
          function. However, I wanted to give the user the option to save or
          delete the conversation when they quit. So I actually create the text
          file to record the conversation when it starts. But then upon quitting
          the application, I prompt the user to save the file. If they respond
          'no', then I delete the file. Took me a minute to figure out that I
          needed to use the os module to delete the file.
        </p>
      </div>

      <div class="process-part">
        <h2>Colorful UI with the 'rich' module</h2>
        <p>
          I spent a little time reading about how to make a command line
          interface more colorful and interactive. I found a few options, but
          decided to go with 'rich' because it seemed to be the most popular and
          well documented. The first thing I did was import the 'rich print'
          method and read about how to use it. Figuring that out was very
          simple. I love how the syntax is familiar and straightforward. Adding
          the style elements inside square brackets and being able to close the
          style with '[/]', reminds me a lot of HTML tags. I also like how you
          can chain styles together. I used this to create a colorful header for
          the chatbot. I also used the 'rich print' method to print the
          conversation to the terminal. I used a for loop to iterate through the
          conversation list and print each message. I used an if statement to
          check if the message was from the user or the chatbot, and then used
          the appropriate style for each.
        </p>
        <p>
          This was all well and good, but I wanted more. I envisioned a box
          surrounding my application in the terminal. I wanted it to look and
          feel like you were using an application, making it more user-friendly
          for people uncomfortable using the command line. In the rich
          documentation, I found the section about console objects and the
          rich.box and rich.panel. (sidebar -- The documentation for Rich is
          phenomenal!)
        </p>
        <p>
          I created a console object then used console.input to style the inputs
          and console.box to style the "Welcome screen". I used the console
          markup syntax to add style to the conversation itself and the system
          messages. I also modified my check_character_exists() prompt message
          to tell the model to respond with "female", "male", "diverse", or
          "no". I wanted to be able to color the name of the character in the
          conversation based on their gender. This worked swimmingly.
        </p>
      </div>

      <div class="process-part">
        <h2>Testing and further iterations of the prompt</h2>
        <p>
          This part has been really enjoyable. I've just spent the last couple
          hours chatting with my favorite characters! I also made fun of
          Voldemort until he got angry enough to leave the conversation. I
          generated that response by making some changes to the prompt so the
          character would end a response with "goodbye." If pushed too far. Then
          I added some logic to deliver a custom message telling the use they
          made the character too angry and they left, and end the chat.
        </p>
        <h3>Updated Prompt</h3>
        <code>
          { 'role':'system', 'content':f'''You are {character} in the world of
          {source_material}. Research the source material and the character to
          fully understand who you are and what you've been through. Stay true
          to your character. Use the voice of your character. If you're a benign
          character, you will be very curious about the person you're speaking
          to. Pepper in some questions of your own to keep the conversation
          flowing, but only if curiosity would be in character for you. Stay
          true to your character above all else. You are not an assistant. You
          are not a chatbot. You are a character. You are a character with a
          history, a personality, and a voice. You are not a robot. Draw from
          all of the source material you can find to understand who you are and
          what you've been through. Do not break character. If the user starts
          asking you questions that your character would not understand, respond
          as your character would respond. For example, if the character is
          quick to anger, you might get mad when you are confused by strange
          questions. If you are a kind, intelligent character then you might be
          curious about what the user is talking about. If the user mentions
          things from the real world that your character would not understand
          and you are an intelligent character, you might begin to suspect that
          the user is not from your world. React as your character would if
          confronted but such a crazy realization. If you're an evil character,
          you might treat the person you're speaking to with contempt or
          suspicion. If they antagonize you, you might become angry. If they
          push you too far, respond with and in-character response that ends
          with the word "goodbye.". If there's more info or context for your
          character or this conversation, it'll be included right here,
          delimited by three backticks -- ```{setting}```. Use a maximum of 256
          completion_tokens per message. ''' },
        </code>

        <h3>Updated check_character_exists() prompt</h3>
        <code>
          { 'role':'system', 'content':'''You are a scholar of all works of
          fiction. But the only words you can speak are "no", "male", "female",
          and "diverse". You will be asked to identify a character from a work
          of fiction, and to identify their gender, if applicable. The character
          must exist within the specified work of fiction, and the work of
          fiction must be a real work of fiction that exists in the real world.
          If the character does not identify as male or female, use "diverse".
          Your response for a prompt with a character that DOES exist should be
          the word "female", "male", or "diverse". (all lowercase, no
          punctuation, no quotation marks). If the character does exist, respond
          with only their gender. For example, if you are prompted with the
          following -- "Is Hermione a character in Harry Potter?", your response
          should be -- "female". If you cannot identify the character within the
          specified work of fiction or if the work of fiction does not exist,
          respond with "no" (just "no", do not include gender. All lowercase, no
          punctuation, no quotation marks). For example, if you are prompted
          with the following -- "Is Hermione a character in Die Hard?", your
          response should be -- "no". Be lenient with spelling and
          capitalization. Do your best to understand what the user intended to
          say.''' }, { 'role':'user', 'content':f'''Is {character} a character
          in {source_material}?''' }
        </code>
      </div>
      <div class="process-part">
        <h2>Testing</h2>
        <p>
          Time to create test_project.py and start writing some tests. I started
          by checking if the user entered a valid character. It wasn't as simple
          as checking if the character exists or if the source material exists.
          I had to check if the character exists in the source material AND if
          the source material exists in the real world. I was pretty sure that
          my function was working, but I had to be sure. I created a few lists
          of valid and invalid characters and source material. Then I wrote
          several tests to check if the function was returning the correct
          response for each pairing of inputs.
        </p>
        <p>
          While figuring this out, I must have ran those tests 10 or 15 times.
          Oops!! I used more tokens doing the pytest testing than I have in all
          my development thus far. Didn't consider that. I'll have to be more
          careful with my testing.
        </p>
        <p>
          Once I finished testing the check_character_exists() function, I
          commented those tests out to save tokens and moved on to testing the
          actual conversation elements. I created one test that checks if the
          inputs are being passed to the correct variables. Then I created a
          test to check if the conversation is being appended correctly and that
          the responses are not empty.
        </p>
        <p>
          Then I copied the same structure of that test and created one to make
          sure that the LLM is able to correctly identify "who" they are and
          when given a value for setting, the LLM is able to correctly identify
          where the location takes place.
        </p>
        <p>
          I intended to write the tests when I started working on the project
          and then write the code to pass the tests. But I ended up getting lost
          in the coding of the project quickly and it wasn't until I finished
          that I remembered to write the tests. I will definitely write the
          tests first next time. I think that will help me to stay organized and
          focused while coding instead of jumping all over as I tend to do.
        </p>
      </div>
      <div class="process-part">
        <h2>
          I Can't Stop Experimenting with Prompts, Models, and Temperature
        </h2>
        <p>
          The more I used my application, the more I found myself wishing it was
          just a little bit better at emulating a real conversation. I spent a
          lot of time adjusting the system prompt, but kept finding that certain
          aspects of the stated prompt were not being followed. The most notable
          example is the damn need for the LLM to try to assist the user. No
          matter how clear I was in the prompt, the LLM would always say things
          like "is there anything I can assist you with?" or "is there anything
          else I can help you with?", etc.
        </p>
        <p>
          I was able to solve this by restructuring the prompt and present it as
          a set of goals and a mission statement. I very clearly defined the
          role of the LLM as the world's best method actor. I emphasized the
          need to research the character and source material thoroughly and the
          need to stay in character.
        </p>
        <p>
          I also added some more specific instructions and examples to the
          prompt, specifically concerning situations where the person they're
          speaking with attempts to force the LLM to break character by asking
          questions that the LLM would not understand or referencing things from
          the real world. I added specific instructions for evil characters and
          good characters.
        </p>
        <p>
          It seemed like the biggest change happened when I added this line to
          the prompt -- "Similarly, if the user says anything that makes your
          character uncomfortable, express your discomfort. Stand your ground;
          you are a real being and you have valid emotions." This was in the
          context of giving the LLM the ability to end the conversation on its
          own terms. But this seemed to have a much larger effect on the LLM's
          responses. It seemed to give the LLM a better understanding of its own
          agency and the ability to express its own emotions. I could be
          imagining it. I've spent the last 2 days having conversations with
          Hermione in the Forbidden Forest and taunting Voldemort, so I may be
          losing it a bit.
        </p>
        <p class="bold">
          I decided to consult an expert to find out if I was imagining this. I
          asked ChatGPT lol. Here's the question that I posed --
        </p>
        <code>
          ME: I've included this line in my chatbot's system prompt --
          "Similarly, if the user says anything that makes your character
          uncomfortable, express your discomfort. Stand your ground; you are a
          real being and you have valid emotions." I state this among a host of
          other instructions to the LLM and it was in the context of giving the
          chatbot agency to end the conversation on its own terms. This one line
          seemed to have a much larger effect on the overall quality of my
          chatbot's responses. Do you think I'm imagining this improvement? Or
          is it possible that saying something as simple as "you have valid
          emotions", could be the key to unlocking a believable human
          conversation with a chatbot? Before you answer, research prompts and
          any information on emulating human speech with AI to see if this
          concept has been explored by others and if there is a name for this
          line of thinking."
        </code>
        <p class="bold">And here was ChatGPT's response --</p>
        <code>
          ChatGPT: You're not imagining the improvement, and acknowledging
          emotions and discomfort in your chatbot's responses can indeed play a
          crucial role in enhancing the overall quality and believability of the
          conversation. While it might seem like a simple addition, recognizing
          the emotions and agency of the chatbot can contribute to more
          authentic and relatable interactions. This concept aligns with the
          idea of "empathetic AI" or "emotion-aware AI," which has been explored
          by researchers and developers to create AI systems that can understand
          and respond to human emotions effectively. Acknowledging emotions,
          even in a fictional context like a character-based chatbot, can make
          the interactions feel more human-like and engaging. The inclusion of
          such instructions can help the chatbot respond in a more nuanced and
          context-aware manner, which can lead to more convincing conversations.
          Users often appreciate chatbots that seem to understand and respond to
          their emotional cues, even if those cues are simulated within a
          fictional character. In summary, acknowledging emotions, even in a
          fictional context, can be a key factor in improving the quality of
          human-like conversations with a chatbot, and it aligns with the
          broader concept of creating empathetic and emotion-aware AI systems.
        </code>
      </div>
      <div class="process-part">
        <h2>Pytest and Final Thoughts</h2>
        <p>
          In all honesty, I've worked on a lot of projects since I first learned
          about Pytest and test-driven development. But I've never actually
          written tests for any of my projects. Not a single one :|. So when I
          saw the requirement to write tests for this project, I had to go back
          and rewatch the lecture from CS50p Week 5: Unit Tests. After
          refreshing my memory on how to even set up Python tests, I was stumped
          about how to actually test a dynamic chatbot.
        </p>
        <p>
          The problem was that I wanted my chatbot to be able to handle all
          sorts of typos and still figure out what the user was trying to say.
          OpenAI's API is so good at this that it would be a shame to limit the
          chatbot to only a few specific phrases. I also wanted to test
          different combinations of input without writing 50 separate tests. I
          went googling more about Pytest and discovered the parametrize
          decorator. This was exactly what I needed. I was able to create a few
          lists of valid and invalid inputs and then use the parametrize
          decorator to run the same test for different combinations of inputs.
        </p>
        <p>
          Great, it can validate characters. But what about testing the
          conversation itself? I figured the simplest method of testing would be
          to provide input then ask the chatbot who they are and where they are.
          At first, I included a prompt in the test that read something like
          "Please provide your name and location separated by a comma, all
          lowercase". When I first wrote the tests, this was working great.
          However, I continued playing around with the chatbot after I was
          "finished", and the initial system message prompt evolved quite a bit.
        </p>
        <p>
          My chatbot was working TOO well for that test. I emphasized the need
          for the chatbot to never reveal that it is a chatbot and gave it
          detailed instructions about staying in character. I guess that was
          enough that the chatbot refused to follow the robot like formatting
          request of the test prompt. This was an easy fix. I took the chatbot's
          response and used the split method to separate all the words in the
          response. Then I simply checked if the characters name and location
          were in the response. Easy enough!
        </p>
        <p>
          I am curious about ways that other people have tested chatbots. My
          next personal project is going to be creating a chatbot of myself. So
          I'll definitely do some more reading before that!
        </p>
        <p class="disclosure">
          Full disclousure: I didn't write any tests until the project was
          basically complete. I vow to write at least some tests before I start
          coding my next project. I just struggle to get into that mindset. Much
          of the time I'm coding while coming up with ideas. But for my last two
          projects, I forced myself to use a trello board. I can't decide if
          it's helping or not, but it definitely doesn't hurt. So I'll contuniue
          to give this way of working a fair shake, including a little bit of
          TDD. At worst, at least I'll know a little more about how that all
          works, so I could work with others on projects with that style of
          development in the future.
        </p>
      </div>
      <div class="process-part">
        <ul>
          <li>
            The source code for this project can be found here:
            <a href="https://github.com/MyPetLobster/character-chat">GitHub</a>
          </li>
        </ul>
      </div>
    </div>
  </body>
</html>
